{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install flair &> /dev/null","metadata":{"execution":{"iopub.status.busy":"2022-11-14T10:00:43.182465Z","iopub.execute_input":"2022-11-14T10:00:43.183338Z","iopub.status.idle":"2022-11-14T10:01:21.437262Z","shell.execute_reply.started":"2022-11-14T10:00:43.183219Z","shell.execute_reply":"2022-11-14T10:01:21.436010Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from glob import glob\ndataset = glob(\"../input/twitter-loc/gold-random-json/*\")","metadata":{"execution":{"iopub.status.busy":"2022-11-14T10:01:21.439665Z","iopub.execute_input":"2022-11-14T10:01:21.440323Z","iopub.status.idle":"2022-11-14T10:01:21.461719Z","shell.execute_reply.started":"2022-11-14T10:01:21.440276Z","shell.execute_reply":"2022-11-14T10:01:21.460715Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"import json\nfrom itertools import product\nfrom flair.data import Sentence\nfrom flair.tokenization import TokenizerWrapper\nfrom nltk import wordpunct_tokenize\n\ntrain_corpus = []\ndev_corpus = []\n\nfor folder,corpus_type in product(dataset,[\"train\",\"dev\"]):\n    for line in open(folder + '/' + corpus_type + '.jsonl'):\n        obj = json.loads(line)\n        sentence = Sentence(obj[\"text\"],use_tokenizer = TokenizerWrapper(wordpunct_tokenize))\n\n        for span_labels in obj[\"location_mentions\"]:\n            s,e = span_labels[\"start_offset\"],span_labels[\"end_offset\"]\n            s_tok,e_tok = -1,-1\n            for i,x in zip(range(len(sentence)),sentence):\n                if s <= x.start_pos and x.end_pos <= e :\n                    s_tok = s_tok if s_tok != -1 else i\n                    e_tok = i\n            try:\n                sentence[s_tok:e_tok + 1].add_label(\"ner\",span_labels[\"type\"])  \n            except:\n                pass\n        globals()[corpus_type + '_corpus'].append(sentence)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-11-14T10:01:21.462941Z","iopub.execute_input":"2022-11-14T10:01:21.463198Z","iopub.status.idle":"2022-11-14T10:01:35.727632Z","shell.execute_reply.started":"2022-11-14T10:01:21.463173Z","shell.execute_reply":"2022-11-14T10:01:35.726394Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"from flair.models import SequenceTagger\nfrom flair.embeddings import WordEmbeddings,FlairEmbeddings,StackedEmbeddings,TransformerWordEmbeddings\nfrom flair.trainers import ModelTrainer\nfrom flair.data import Corpus\n\ncorpus = Corpus(train_corpus,dev_corpus) # store train and test data\ncorpus.downsample(0.35) #downsample to 35% ","metadata":{"execution":{"iopub.status.busy":"2022-11-14T10:01:35.730696Z","iopub.execute_input":"2022-11-14T10:01:35.731638Z","iopub.status.idle":"2022-11-14T10:01:35.776303Z","shell.execute_reply.started":"2022-11-14T10:01:35.731600Z","shell.execute_reply":"2022-11-14T10:01:35.775300Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<flair.data.Corpus at 0x7f59c6207710>"},"metadata":{}}]},{"cell_type":"markdown","source":"### GLOVE + BiLSTM + CRF","metadata":{}},{"cell_type":"code","source":"label_type = 'ner' #Name-Entity Recognition (NER): It can recognise whether a \n                    #word represents a person, location or names in the text.\n\n\nlabel_dict = corpus.make_label_dictionary(label_type=label_type) #Create a label dictionary from ner and data\nprint(label_dict)\n\nembedding_types = [\n    WordEmbeddings('glove'),\n    FlairEmbeddings('news-forward'),\n    FlairEmbeddings('news-backward'),\n]\n#The three embedding models will be concatenated and should give state of the art results.\n#Document Embeddings generate one embedding for an entire text. \n#The produced embeddings are PyTorch vectors. \n#There are two different methods using the word embeddings to obtain a document embedding\n\nembeddings = StackedEmbeddings(embeddings=embedding_types)\n\ntagger = SequenceTagger(hidden_size=256,\n                        embeddings=embeddings,\n                        tag_dictionary=label_dict,\n                        tag_type=label_type,\n                        use_crf=True)\n\ntrainer = ModelTrainer(tagger, corpus)\n\ntrainer.train('resources/taggers/sota-ner-flair',\n              learning_rate=0.08,\n              mini_batch_size=32,\n              max_epochs=16)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T10:01:35.777792Z","iopub.execute_input":"2022-11-14T10:01:35.778419Z","iopub.status.idle":"2022-11-14T10:08:52.691046Z","shell.execute_reply.started":"2022-11-14T10:01:35.778382Z","shell.execute_reply":"2022-11-14T10:08:52.690033Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"2022-11-14 10:01:35,781 Computing label dictionary. Progress:\n","output_type":"stream"},{"name":"stderr","text":"4534it [00:00, 51169.32it/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:01:35,878 Dictionary created for label 'ner' with 13 values: Country (seen 1618 times), State (seen 1338 times), City/town (seen 1103 times), Island (seen 324 times), County (seen 90 times), Human-made Point-of-Interest (seen 64 times), District (seen 54 times), Natural Point-of-Interest (seen 42 times), Continent (seen 27 times), Neighborhood (seen 19 times), Road/street (seen 19 times), Other locations (seen 15 times)\nDictionary with 13 tags: <unk>, Country, State, City/town, Island, County, Human-made Point-of-Interest, District, Natural Point-of-Interest, Continent, Neighborhood, Road/street, Other locations\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:01:36,064 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmp9e7sa6ox\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 160000128/160000128 [00:04<00:00, 37761289.26B/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:01:40,483 copying /tmp/tmp9e7sa6ox to cache at /root/.flair/embeddings/glove.gensim.vectors.npy\n2022-11-14 10:01:40,659 removing temp file /tmp/tmp9e7sa6ox\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:01:40,860 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim not found in cache, downloading to /tmp/tmp0xkydsr1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 21494764/21494764 [00:00<00:00, 36385654.66B/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:01:41,620 copying /tmp/tmp0xkydsr1 to cache at /root/.flair/embeddings/glove.gensim\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:01:41,648 removing temp file /tmp/tmp0xkydsr1\n2022-11-14 10:01:46,581 https://flair.informatik.hu-berlin.de/resources/embeddings/flair/news-forward-0.4.1.pt not found in cache, downloading to /tmp/tmpf6ckilmj\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 73034624/73034624 [00:01<00:00, 36793745.11B/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:01:48,731 copying /tmp/tmpf6ckilmj to cache at /root/.flair/embeddings/news-forward-0.4.1.pt\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:01:48,814 removing temp file /tmp/tmpf6ckilmj\n2022-11-14 10:01:56,079 https://flair.informatik.hu-berlin.de/resources/embeddings/flair/news-backward-0.4.1.pt not found in cache, downloading to /tmp/tmp_ouezfud\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 73034575/73034575 [00:02<00:00, 35220196.65B/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:01:58,319 copying /tmp/tmp_ouezfud to cache at /root/.flair/embeddings/news-backward-0.4.1.pt\n2022-11-14 10:01:58,399 removing temp file /tmp/tmp_ouezfud\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:01:58,658 SequenceTagger predicts: Dictionary with 49 tags: O, S-Country, B-Country, E-Country, I-Country, S-State, B-State, E-State, I-State, S-City/town, B-City/town, E-City/town, I-City/town, S-Island, B-Island, E-Island, I-Island, S-County, B-County, E-County, I-County, S-Human-made Point-of-Interest, B-Human-made Point-of-Interest, E-Human-made Point-of-Interest, I-Human-made Point-of-Interest, S-District, B-District, E-District, I-District, S-Natural Point-of-Interest, B-Natural Point-of-Interest, E-Natural Point-of-Interest, I-Natural Point-of-Interest, S-Continent, B-Continent, E-Continent, I-Continent, S-Neighborhood, B-Neighborhood, E-Neighborhood, I-Neighborhood, S-Road/street, B-Road/street, E-Road/street, I-Road/street, S-Other locations, B-Other locations, E-Other locations, I-Other locations\n2022-11-14 10:01:58,931 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:01:58,932 Model: \"SequenceTagger(\n  (embeddings): StackedEmbeddings(\n    (list_embedding_0): WordEmbeddings(\n      'glove'\n      (embedding): Embedding(400001, 100)\n    )\n    (list_embedding_1): FlairEmbeddings(\n      (lm): LanguageModel(\n        (drop): Dropout(p=0.05, inplace=False)\n        (encoder): Embedding(300, 100)\n        (rnn): LSTM(100, 2048)\n        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n      )\n    )\n    (list_embedding_2): FlairEmbeddings(\n      (lm): LanguageModel(\n        (drop): Dropout(p=0.05, inplace=False)\n        (encoder): Embedding(300, 100)\n        (rnn): LSTM(100, 2048)\n        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n      )\n    )\n  )\n  (word_dropout): WordDropout(p=0.05)\n  (locked_dropout): LockedDropout(p=0.5)\n  (embedding2nn): Linear(in_features=4196, out_features=4196, bias=True)\n  (rnn): LSTM(4196, 256, batch_first=True, bidirectional=True)\n  (linear): Linear(in_features=512, out_features=51, bias=True)\n  (loss_function): ViterbiLoss()\n  (crf): CRF()\n)\"\n2022-11-14 10:01:58,933 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:01:58,935 Corpus: \"Corpus: 4534 train + 720 dev + 504 test sentences\"\n2022-11-14 10:01:58,936 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:01:58,937 Parameters:\n2022-11-14 10:01:58,938  - learning_rate: \"0.080000\"\n2022-11-14 10:01:58,939  - mini_batch_size: \"32\"\n2022-11-14 10:01:58,940  - patience: \"3\"\n2022-11-14 10:01:58,941  - anneal_factor: \"0.5\"\n2022-11-14 10:01:58,942  - max_epochs: \"16\"\n2022-11-14 10:01:58,943  - shuffle: \"True\"\n2022-11-14 10:01:58,944  - train_with_dev: \"False\"\n2022-11-14 10:01:58,945  - batch_growth_annealing: \"False\"\n2022-11-14 10:01:58,946 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:01:58,946 Model training base path: \"resources/taggers/sota-ner-flair\"\n2022-11-14 10:01:58,947 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:01:58,948 Device: cuda:0\n2022-11-14 10:01:58,949 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:01:58,950 Embeddings storage mode: cpu\n2022-11-14 10:01:58,951 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:02:01,527 epoch 1 - iter 14/142 - loss 1.07781429 - samples/sec: 174.07 - lr: 0.080000\n2022-11-14 10:02:04,623 epoch 1 - iter 28/142 - loss 0.62093681 - samples/sec: 144.79 - lr: 0.080000\n2022-11-14 10:02:08,860 epoch 1 - iter 42/142 - loss 0.45688788 - samples/sec: 105.81 - lr: 0.080000\n2022-11-14 10:02:11,510 epoch 1 - iter 56/142 - loss 0.40393290 - samples/sec: 169.21 - lr: 0.080000\n2022-11-14 10:02:14,088 epoch 1 - iter 70/142 - loss 0.37819423 - samples/sec: 173.95 - lr: 0.080000\n2022-11-14 10:02:17,488 epoch 1 - iter 84/142 - loss 0.34807661 - samples/sec: 131.83 - lr: 0.080000\n2022-11-14 10:02:21,774 epoch 1 - iter 98/142 - loss 0.31323079 - samples/sec: 104.58 - lr: 0.080000\n2022-11-14 10:02:26,394 epoch 1 - iter 112/142 - loss 0.28661875 - samples/sec: 97.04 - lr: 0.080000\n2022-11-14 10:02:30,079 epoch 1 - iter 126/142 - loss 0.27103388 - samples/sec: 121.64 - lr: 0.080000\n2022-11-14 10:02:33,252 epoch 1 - iter 140/142 - loss 0.26453161 - samples/sec: 141.32 - lr: 0.080000\n2022-11-14 10:02:33,901 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:02:33,902 EPOCH 1 done: loss 0.2619 - lr 0.080000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 23/23 [00:07<00:00,  3.20it/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:02:41,093 Evaluating as a multi-label problem: False\n2022-11-14 10:02:41,108 DEV : loss 0.15231506526470184 - f1-score (micro avg)  0.538\n2022-11-14 10:02:41,190 BAD EPOCHS (no improvement): 0\n2022-11-14 10:02:41,192 saving best model\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:02:42,775 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:02:44,649 epoch 2 - iter 14/142 - loss 0.14358425 - samples/sec: 239.63 - lr: 0.080000\n2022-11-14 10:02:46,488 epoch 2 - iter 28/142 - loss 0.14758484 - samples/sec: 243.86 - lr: 0.080000\n2022-11-14 10:02:48,270 epoch 2 - iter 42/142 - loss 0.15304072 - samples/sec: 251.92 - lr: 0.080000\n2022-11-14 10:02:50,001 epoch 2 - iter 56/142 - loss 0.14468650 - samples/sec: 259.18 - lr: 0.080000\n2022-11-14 10:02:52,280 epoch 2 - iter 70/142 - loss 0.13846654 - samples/sec: 197.60 - lr: 0.080000\n2022-11-14 10:02:54,033 epoch 2 - iter 84/142 - loss 0.13627222 - samples/sec: 255.88 - lr: 0.080000\n2022-11-14 10:02:55,808 epoch 2 - iter 98/142 - loss 0.13372834 - samples/sec: 252.88 - lr: 0.080000\n2022-11-14 10:02:58,022 epoch 2 - iter 112/142 - loss 0.13185661 - samples/sec: 202.54 - lr: 0.080000\n2022-11-14 10:02:59,867 epoch 2 - iter 126/142 - loss 0.12998564 - samples/sec: 243.28 - lr: 0.080000\n2022-11-14 10:03:01,670 epoch 2 - iter 140/142 - loss 0.12854792 - samples/sec: 248.82 - lr: 0.080000\n2022-11-14 10:03:01,899 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:03:01,900 EPOCH 2 done: loss 0.1284 - lr 0.080000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 23/23 [00:03<00:00,  6.35it/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:03:05,533 Evaluating as a multi-label problem: False\n2022-11-14 10:03:05,547 DEV : loss 0.09187287092208862 - f1-score (micro avg)  0.7531\n2022-11-14 10:03:05,629 BAD EPOCHS (no improvement): 0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:03:05,631 saving best model\n2022-11-14 10:03:07,519 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:03:09,373 epoch 3 - iter 14/142 - loss 0.11118566 - samples/sec: 242.65 - lr: 0.080000\n2022-11-14 10:03:11,204 epoch 3 - iter 28/142 - loss 0.10677438 - samples/sec: 245.06 - lr: 0.080000\n2022-11-14 10:03:13,042 epoch 3 - iter 42/142 - loss 0.10981308 - samples/sec: 244.02 - lr: 0.080000\n2022-11-14 10:03:14,950 epoch 3 - iter 56/142 - loss 0.11186415 - samples/sec: 235.23 - lr: 0.080000\n2022-11-14 10:03:16,752 epoch 3 - iter 70/142 - loss 0.11325270 - samples/sec: 248.88 - lr: 0.080000\n2022-11-14 10:03:18,557 epoch 3 - iter 84/142 - loss 0.10966481 - samples/sec: 248.62 - lr: 0.080000\n2022-11-14 10:03:20,308 epoch 3 - iter 98/142 - loss 0.10569062 - samples/sec: 256.19 - lr: 0.080000\n2022-11-14 10:03:22,105 epoch 3 - iter 112/142 - loss 0.10651569 - samples/sec: 249.73 - lr: 0.080000\n2022-11-14 10:03:23,932 epoch 3 - iter 126/142 - loss 0.10567784 - samples/sec: 245.51 - lr: 0.080000\n2022-11-14 10:03:25,900 epoch 3 - iter 140/142 - loss 0.10445906 - samples/sec: 227.94 - lr: 0.080000\n2022-11-14 10:03:26,162 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:03:26,163 EPOCH 3 done: loss 0.1040 - lr 0.080000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 23/23 [00:03<00:00,  5.94it/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:03:30,046 Evaluating as a multi-label problem: False\n2022-11-14 10:03:30,061 DEV : loss 0.0807940810918808 - f1-score (micro avg)  0.779\n2022-11-14 10:03:30,143 BAD EPOCHS (no improvement): 0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:03:30,145 saving best model\n2022-11-14 10:03:32,031 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:03:33,871 epoch 4 - iter 14/142 - loss 0.10651883 - samples/sec: 244.08 - lr: 0.080000\n2022-11-14 10:03:35,918 epoch 4 - iter 28/142 - loss 0.09368855 - samples/sec: 219.16 - lr: 0.080000\n2022-11-14 10:03:37,786 epoch 4 - iter 42/142 - loss 0.09191476 - samples/sec: 240.23 - lr: 0.080000\n2022-11-14 10:03:39,650 epoch 4 - iter 56/142 - loss 0.09375563 - samples/sec: 240.65 - lr: 0.080000\n2022-11-14 10:03:41,435 epoch 4 - iter 70/142 - loss 0.09100946 - samples/sec: 251.36 - lr: 0.080000\n2022-11-14 10:03:43,314 epoch 4 - iter 84/142 - loss 0.09007810 - samples/sec: 238.81 - lr: 0.080000\n2022-11-14 10:03:45,066 epoch 4 - iter 98/142 - loss 0.09111849 - samples/sec: 256.01 - lr: 0.080000\n2022-11-14 10:03:47,043 epoch 4 - iter 112/142 - loss 0.09135131 - samples/sec: 226.95 - lr: 0.080000\n2022-11-14 10:03:48,787 epoch 4 - iter 126/142 - loss 0.09159176 - samples/sec: 257.33 - lr: 0.080000\n2022-11-14 10:03:50,493 epoch 4 - iter 140/142 - loss 0.09288350 - samples/sec: 262.93 - lr: 0.080000\n2022-11-14 10:03:50,703 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:03:50,704 EPOCH 4 done: loss 0.0926 - lr 0.080000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 23/23 [00:04<00:00,  5.64it/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:03:54,795 Evaluating as a multi-label problem: False\n2022-11-14 10:03:54,809 DEV : loss 0.07420500367879868 - f1-score (micro avg)  0.79\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:03:54,892 BAD EPOCHS (no improvement): 0\n2022-11-14 10:03:54,894 saving best model\n2022-11-14 10:03:56,783 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:03:58,810 epoch 5 - iter 14/142 - loss 0.09178285 - samples/sec: 221.51 - lr: 0.080000\n2022-11-14 10:04:00,932 epoch 5 - iter 28/142 - loss 0.09209837 - samples/sec: 211.44 - lr: 0.080000\n2022-11-14 10:04:02,792 epoch 5 - iter 42/142 - loss 0.08555943 - samples/sec: 241.66 - lr: 0.080000\n2022-11-14 10:04:04,597 epoch 5 - iter 56/142 - loss 0.08336726 - samples/sec: 248.47 - lr: 0.080000\n2022-11-14 10:04:06,324 epoch 5 - iter 70/142 - loss 0.08487884 - samples/sec: 259.92 - lr: 0.080000\n2022-11-14 10:04:08,178 epoch 5 - iter 84/142 - loss 0.08794307 - samples/sec: 241.93 - lr: 0.080000\n2022-11-14 10:04:10,146 epoch 5 - iter 98/142 - loss 0.08737433 - samples/sec: 228.02 - lr: 0.080000\n2022-11-14 10:04:11,924 epoch 5 - iter 112/142 - loss 0.08603128 - samples/sec: 252.31 - lr: 0.080000\n2022-11-14 10:04:13,717 epoch 5 - iter 126/142 - loss 0.08689121 - samples/sec: 250.22 - lr: 0.080000\n2022-11-14 10:04:15,563 epoch 5 - iter 140/142 - loss 0.08542189 - samples/sec: 243.01 - lr: 0.080000\n2022-11-14 10:04:15,791 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:04:15,792 EPOCH 5 done: loss 0.0852 - lr 0.080000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 23/23 [00:03<00:00,  6.44it/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:04:19,379 Evaluating as a multi-label problem: False\n2022-11-14 10:04:19,410 DEV : loss 0.06877637654542923 - f1-score (micro avg)  0.798\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:04:19,561 BAD EPOCHS (no improvement): 0\n2022-11-14 10:04:19,564 saving best model\n2022-11-14 10:04:21,488 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:04:23,328 epoch 6 - iter 14/142 - loss 0.08334105 - samples/sec: 244.67 - lr: 0.080000\n2022-11-14 10:04:25,101 epoch 6 - iter 28/142 - loss 0.07973664 - samples/sec: 252.94 - lr: 0.080000\n2022-11-14 10:04:26,914 epoch 6 - iter 42/142 - loss 0.08182138 - samples/sec: 247.57 - lr: 0.080000\n2022-11-14 10:04:28,785 epoch 6 - iter 56/142 - loss 0.08122394 - samples/sec: 239.71 - lr: 0.080000\n2022-11-14 10:04:30,758 epoch 6 - iter 70/142 - loss 0.07996460 - samples/sec: 227.37 - lr: 0.080000\n2022-11-14 10:04:33,025 epoch 6 - iter 84/142 - loss 0.08135600 - samples/sec: 198.14 - lr: 0.080000\n2022-11-14 10:04:34,954 epoch 6 - iter 98/142 - loss 0.08133630 - samples/sec: 232.55 - lr: 0.080000\n2022-11-14 10:04:36,801 epoch 6 - iter 112/142 - loss 0.07954041 - samples/sec: 242.92 - lr: 0.080000\n2022-11-14 10:04:38,751 epoch 6 - iter 126/142 - loss 0.08031171 - samples/sec: 230.03 - lr: 0.080000\n2022-11-14 10:04:40,487 epoch 6 - iter 140/142 - loss 0.08002298 - samples/sec: 258.60 - lr: 0.080000\n2022-11-14 10:04:40,729 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:04:40,730 EPOCH 6 done: loss 0.0801 - lr 0.080000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 23/23 [00:03<00:00,  6.28it/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:04:44,401 Evaluating as a multi-label problem: False\n2022-11-14 10:04:44,416 DEV : loss 0.06611115485429764 - f1-score (micro avg)  0.8064\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:04:44,497 BAD EPOCHS (no improvement): 0\n2022-11-14 10:04:44,499 saving best model\n2022-11-14 10:04:46,344 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:04:48,228 epoch 7 - iter 14/142 - loss 0.07866721 - samples/sec: 238.40 - lr: 0.080000\n2022-11-14 10:04:50,054 epoch 7 - iter 28/142 - loss 0.07027915 - samples/sec: 246.15 - lr: 0.080000\n2022-11-14 10:04:51,864 epoch 7 - iter 42/142 - loss 0.07297193 - samples/sec: 247.98 - lr: 0.080000\n2022-11-14 10:04:53,891 epoch 7 - iter 56/142 - loss 0.07214250 - samples/sec: 221.23 - lr: 0.080000\n2022-11-14 10:04:55,720 epoch 7 - iter 70/142 - loss 0.07339500 - samples/sec: 245.34 - lr: 0.080000\n2022-11-14 10:04:57,588 epoch 7 - iter 84/142 - loss 0.07375202 - samples/sec: 240.27 - lr: 0.080000\n2022-11-14 10:04:59,414 epoch 7 - iter 98/142 - loss 0.07396623 - samples/sec: 245.68 - lr: 0.080000\n2022-11-14 10:05:01,222 epoch 7 - iter 112/142 - loss 0.07455242 - samples/sec: 248.08 - lr: 0.080000\n2022-11-14 10:05:03,142 epoch 7 - iter 126/142 - loss 0.07568613 - samples/sec: 233.71 - lr: 0.080000\n2022-11-14 10:05:05,487 epoch 7 - iter 140/142 - loss 0.07434703 - samples/sec: 191.47 - lr: 0.080000\n2022-11-14 10:05:05,704 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:05:05,705 EPOCH 7 done: loss 0.0748 - lr 0.080000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 23/23 [00:04<00:00,  5.56it/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:05:09,855 Evaluating as a multi-label problem: False\n2022-11-14 10:05:09,869 DEV : loss 0.06359512358903885 - f1-score (micro avg)  0.8236\n2022-11-14 10:05:09,951 BAD EPOCHS (no improvement): 0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:05:09,953 saving best model\n2022-11-14 10:05:11,821 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:05:13,640 epoch 8 - iter 14/142 - loss 0.07063536 - samples/sec: 246.94 - lr: 0.080000\n2022-11-14 10:05:15,633 epoch 8 - iter 28/142 - loss 0.07246922 - samples/sec: 225.06 - lr: 0.080000\n2022-11-14 10:05:17,570 epoch 8 - iter 42/142 - loss 0.07603410 - samples/sec: 231.69 - lr: 0.080000\n2022-11-14 10:05:19,412 epoch 8 - iter 56/142 - loss 0.07445316 - samples/sec: 243.50 - lr: 0.080000\n2022-11-14 10:05:21,151 epoch 8 - iter 70/142 - loss 0.07329504 - samples/sec: 257.99 - lr: 0.080000\n2022-11-14 10:05:23,012 epoch 8 - iter 84/142 - loss 0.07300103 - samples/sec: 241.14 - lr: 0.080000\n2022-11-14 10:05:24,791 epoch 8 - iter 98/142 - loss 0.07339363 - samples/sec: 252.13 - lr: 0.080000\n2022-11-14 10:05:26,776 epoch 8 - iter 112/142 - loss 0.07267391 - samples/sec: 226.11 - lr: 0.080000\n2022-11-14 10:05:28,503 epoch 8 - iter 126/142 - loss 0.07140839 - samples/sec: 259.87 - lr: 0.080000\n2022-11-14 10:05:30,356 epoch 8 - iter 140/142 - loss 0.07129851 - samples/sec: 242.04 - lr: 0.080000\n2022-11-14 10:05:30,592 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:05:30,593 EPOCH 8 done: loss 0.0714 - lr 0.080000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 23/23 [00:03<00:00,  6.50it/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:05:34,143 Evaluating as a multi-label problem: False\n2022-11-14 10:05:34,159 DEV : loss 0.06726896017789841 - f1-score (micro avg)  0.8141\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:05:34,242 BAD EPOCHS (no improvement): 1\n2022-11-14 10:05:34,244 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:05:36,584 epoch 9 - iter 14/142 - loss 0.06983580 - samples/sec: 192.00 - lr: 0.080000\n2022-11-14 10:05:38,561 epoch 9 - iter 28/142 - loss 0.06832488 - samples/sec: 227.13 - lr: 0.080000\n2022-11-14 10:05:40,372 epoch 9 - iter 42/142 - loss 0.07108264 - samples/sec: 247.80 - lr: 0.080000\n2022-11-14 10:05:42,160 epoch 9 - iter 56/142 - loss 0.07056382 - samples/sec: 250.85 - lr: 0.080000\n2022-11-14 10:05:43,979 epoch 9 - iter 70/142 - loss 0.06913719 - samples/sec: 246.68 - lr: 0.080000\n2022-11-14 10:05:45,762 epoch 9 - iter 84/142 - loss 0.06903578 - samples/sec: 251.71 - lr: 0.080000\n2022-11-14 10:05:47,657 epoch 9 - iter 98/142 - loss 0.06926506 - samples/sec: 236.68 - lr: 0.080000\n2022-11-14 10:05:49,560 epoch 9 - iter 112/142 - loss 0.06770569 - samples/sec: 235.77 - lr: 0.080000\n2022-11-14 10:05:51,392 epoch 9 - iter 126/142 - loss 0.06761881 - samples/sec: 244.89 - lr: 0.080000\n2022-11-14 10:05:53,251 epoch 9 - iter 140/142 - loss 0.06810842 - samples/sec: 241.31 - lr: 0.080000\n2022-11-14 10:05:53,486 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:05:53,487 EPOCH 9 done: loss 0.0678 - lr 0.080000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 23/23 [00:03<00:00,  6.62it/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:05:56,974 Evaluating as a multi-label problem: False\n2022-11-14 10:05:56,988 DEV : loss 0.0658353939652443 - f1-score (micro avg)  0.7945\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:05:57,073 BAD EPOCHS (no improvement): 2\n2022-11-14 10:05:57,074 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:05:59,112 epoch 10 - iter 14/142 - loss 0.06192536 - samples/sec: 220.25 - lr: 0.080000\n2022-11-14 10:06:00,942 epoch 10 - iter 28/142 - loss 0.06669605 - samples/sec: 245.12 - lr: 0.080000\n2022-11-14 10:06:02,775 epoch 10 - iter 42/142 - loss 0.06547793 - samples/sec: 244.79 - lr: 0.080000\n2022-11-14 10:06:04,575 epoch 10 - iter 56/142 - loss 0.06539094 - samples/sec: 249.23 - lr: 0.080000\n2022-11-14 10:06:06,561 epoch 10 - iter 70/142 - loss 0.06623971 - samples/sec: 225.91 - lr: 0.080000\n2022-11-14 10:06:08,500 epoch 10 - iter 84/142 - loss 0.06657855 - samples/sec: 231.67 - lr: 0.080000\n2022-11-14 10:06:10,532 epoch 10 - iter 98/142 - loss 0.06696823 - samples/sec: 220.78 - lr: 0.080000\n2022-11-14 10:06:12,352 epoch 10 - iter 112/142 - loss 0.06661700 - samples/sec: 246.60 - lr: 0.080000\n2022-11-14 10:06:14,149 epoch 10 - iter 126/142 - loss 0.06612024 - samples/sec: 249.57 - lr: 0.080000\n2022-11-14 10:06:15,952 epoch 10 - iter 140/142 - loss 0.06539826 - samples/sec: 248.94 - lr: 0.080000\n2022-11-14 10:06:16,164 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:06:16,165 EPOCH 10 done: loss 0.0655 - lr 0.080000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 23/23 [00:04<00:00,  5.37it/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:06:20,461 Evaluating as a multi-label problem: False\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:06:20,485 DEV : loss 0.060830965638160706 - f1-score (micro avg)  0.8309\n2022-11-14 10:06:20,628 BAD EPOCHS (no improvement): 0\n2022-11-14 10:06:20,630 saving best model\n2022-11-14 10:06:22,526 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:06:24,433 epoch 11 - iter 14/142 - loss 0.07226787 - samples/sec: 235.83 - lr: 0.080000\n2022-11-14 10:06:26,258 epoch 11 - iter 28/142 - loss 0.06534408 - samples/sec: 245.91 - lr: 0.080000\n2022-11-14 10:06:28,177 epoch 11 - iter 42/142 - loss 0.06485031 - samples/sec: 233.73 - lr: 0.080000\n2022-11-14 10:06:30,055 epoch 11 - iter 56/142 - loss 0.06357679 - samples/sec: 238.91 - lr: 0.080000\n2022-11-14 10:06:32,051 epoch 11 - iter 70/142 - loss 0.06482482 - samples/sec: 224.68 - lr: 0.080000\n2022-11-14 10:06:33,848 epoch 11 - iter 84/142 - loss 0.06494826 - samples/sec: 249.76 - lr: 0.080000\n2022-11-14 10:06:35,650 epoch 11 - iter 98/142 - loss 0.06379204 - samples/sec: 248.96 - lr: 0.080000\n2022-11-14 10:06:37,456 epoch 11 - iter 112/142 - loss 0.06415702 - samples/sec: 248.36 - lr: 0.080000\n2022-11-14 10:06:39,701 epoch 11 - iter 126/142 - loss 0.06295251 - samples/sec: 199.88 - lr: 0.080000\n2022-11-14 10:06:41,541 epoch 11 - iter 140/142 - loss 0.06323973 - samples/sec: 243.85 - lr: 0.080000\n2022-11-14 10:06:41,783 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:06:41,784 EPOCH 11 done: loss 0.0631 - lr 0.080000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 23/23 [00:04<00:00,  5.34it/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:06:46,102 Evaluating as a multi-label problem: False\n2022-11-14 10:06:46,117 DEV : loss 0.05924300476908684 - f1-score (micro avg)  0.8092\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:06:46,199 BAD EPOCHS (no improvement): 1\n2022-11-14 10:06:46,200 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:06:48,089 epoch 12 - iter 14/142 - loss 0.06206797 - samples/sec: 237.63 - lr: 0.080000\n2022-11-14 10:06:49,978 epoch 12 - iter 28/142 - loss 0.06357106 - samples/sec: 237.46 - lr: 0.080000\n2022-11-14 10:06:51,816 epoch 12 - iter 42/142 - loss 0.06152739 - samples/sec: 244.12 - lr: 0.080000\n2022-11-14 10:06:53,788 epoch 12 - iter 56/142 - loss 0.06095086 - samples/sec: 227.55 - lr: 0.080000\n2022-11-14 10:06:55,628 epoch 12 - iter 70/142 - loss 0.06143311 - samples/sec: 244.18 - lr: 0.080000\n2022-11-14 10:06:57,430 epoch 12 - iter 84/142 - loss 0.06267059 - samples/sec: 249.05 - lr: 0.080000\n2022-11-14 10:06:59,250 epoch 12 - iter 98/142 - loss 0.06024684 - samples/sec: 246.41 - lr: 0.080000\n2022-11-14 10:07:00,975 epoch 12 - iter 112/142 - loss 0.06148390 - samples/sec: 260.17 - lr: 0.080000\n2022-11-14 10:07:02,826 epoch 12 - iter 126/142 - loss 0.06114941 - samples/sec: 242.34 - lr: 0.080000\n2022-11-14 10:07:05,189 epoch 12 - iter 140/142 - loss 0.06116176 - samples/sec: 189.81 - lr: 0.080000\n2022-11-14 10:07:05,522 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:07:05,529 EPOCH 12 done: loss 0.0614 - lr 0.080000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 23/23 [00:03<00:00,  6.11it/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:07:09,308 Evaluating as a multi-label problem: False\n2022-11-14 10:07:09,332 DEV : loss 0.057672467082738876 - f1-score (micro avg)  0.8126\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:07:09,488 BAD EPOCHS (no improvement): 2\n2022-11-14 10:07:09,492 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:07:11,619 epoch 13 - iter 14/142 - loss 0.05901926 - samples/sec: 211.04 - lr: 0.080000\n2022-11-14 10:07:13,450 epoch 13 - iter 28/142 - loss 0.06132986 - samples/sec: 244.98 - lr: 0.080000\n2022-11-14 10:07:15,226 epoch 13 - iter 42/142 - loss 0.05967694 - samples/sec: 252.68 - lr: 0.080000\n2022-11-14 10:07:17,264 epoch 13 - iter 56/142 - loss 0.05966500 - samples/sec: 220.10 - lr: 0.080000\n2022-11-14 10:07:19,165 epoch 13 - iter 70/142 - loss 0.05960490 - samples/sec: 236.09 - lr: 0.080000\n2022-11-14 10:07:20,929 epoch 13 - iter 84/142 - loss 0.06022265 - samples/sec: 254.28 - lr: 0.080000\n2022-11-14 10:07:22,679 epoch 13 - iter 98/142 - loss 0.06055963 - samples/sec: 256.34 - lr: 0.080000\n2022-11-14 10:07:24,535 epoch 13 - iter 112/142 - loss 0.05982460 - samples/sec: 241.80 - lr: 0.080000\n2022-11-14 10:07:26,300 epoch 13 - iter 126/142 - loss 0.05989396 - samples/sec: 254.23 - lr: 0.080000\n2022-11-14 10:07:28,310 epoch 13 - iter 140/142 - loss 0.05891522 - samples/sec: 223.22 - lr: 0.080000\n2022-11-14 10:07:28,534 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:07:28,535 EPOCH 13 done: loss 0.0588 - lr 0.080000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 23/23 [00:04<00:00,  5.58it/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:07:32,670 Evaluating as a multi-label problem: False\n2022-11-14 10:07:32,685 DEV : loss 0.05732309818267822 - f1-score (micro avg)  0.8347\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:07:32,768 BAD EPOCHS (no improvement): 0\n2022-11-14 10:07:32,770 saving best model\n2022-11-14 10:07:34,606 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:07:36,415 epoch 14 - iter 14/142 - loss 0.06302488 - samples/sec: 248.84 - lr: 0.080000\n2022-11-14 10:07:38,474 epoch 14 - iter 28/142 - loss 0.06295571 - samples/sec: 217.86 - lr: 0.080000\n2022-11-14 10:07:40,314 epoch 14 - iter 42/142 - loss 0.05704805 - samples/sec: 244.24 - lr: 0.080000\n2022-11-14 10:07:42,416 epoch 14 - iter 56/142 - loss 0.05725535 - samples/sec: 213.38 - lr: 0.080000\n2022-11-14 10:07:44,275 epoch 14 - iter 70/142 - loss 0.05705646 - samples/sec: 241.33 - lr: 0.080000\n2022-11-14 10:07:46,036 epoch 14 - iter 84/142 - loss 0.05730392 - samples/sec: 254.73 - lr: 0.080000\n2022-11-14 10:07:47,876 epoch 14 - iter 98/142 - loss 0.05650177 - samples/sec: 243.83 - lr: 0.080000\n2022-11-14 10:07:49,928 epoch 14 - iter 112/142 - loss 0.05566882 - samples/sec: 218.61 - lr: 0.080000\n2022-11-14 10:07:51,752 epoch 14 - iter 126/142 - loss 0.05535663 - samples/sec: 245.95 - lr: 0.080000\n2022-11-14 10:07:53,596 epoch 14 - iter 140/142 - loss 0.05590181 - samples/sec: 243.44 - lr: 0.080000\n2022-11-14 10:07:53,824 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:07:53,825 EPOCH 14 done: loss 0.0559 - lr 0.080000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 23/23 [00:03<00:00,  6.65it/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:07:57,297 Evaluating as a multi-label problem: False\n2022-11-14 10:07:57,314 DEV : loss 0.060734156519174576 - f1-score (micro avg)  0.8255\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:07:57,398 BAD EPOCHS (no improvement): 1\n2022-11-14 10:07:57,399 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:07:59,279 epoch 15 - iter 14/142 - loss 0.05927442 - samples/sec: 238.72 - lr: 0.080000\n2022-11-14 10:08:01,291 epoch 15 - iter 28/142 - loss 0.06028537 - samples/sec: 222.99 - lr: 0.080000\n2022-11-14 10:08:03,036 epoch 15 - iter 42/142 - loss 0.05851002 - samples/sec: 257.16 - lr: 0.080000\n2022-11-14 10:08:04,876 epoch 15 - iter 56/142 - loss 0.05619488 - samples/sec: 243.77 - lr: 0.080000\n2022-11-14 10:08:06,734 epoch 15 - iter 70/142 - loss 0.05584564 - samples/sec: 241.55 - lr: 0.080000\n2022-11-14 10:08:08,572 epoch 15 - iter 84/142 - loss 0.05684150 - samples/sec: 244.08 - lr: 0.080000\n2022-11-14 10:08:10,485 epoch 15 - iter 98/142 - loss 0.05667322 - samples/sec: 234.50 - lr: 0.080000\n2022-11-14 10:08:12,534 epoch 15 - iter 112/142 - loss 0.05606869 - samples/sec: 219.22 - lr: 0.080000\n2022-11-14 10:08:14,569 epoch 15 - iter 126/142 - loss 0.05541651 - samples/sec: 220.73 - lr: 0.080000\n2022-11-14 10:08:16,314 epoch 15 - iter 140/142 - loss 0.05539057 - samples/sec: 257.07 - lr: 0.080000\n2022-11-14 10:08:16,533 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:08:16,535 EPOCH 15 done: loss 0.0554 - lr 0.080000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 23/23 [00:03<00:00,  6.61it/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:08:20,027 Evaluating as a multi-label problem: False\n2022-11-14 10:08:20,043 DEV : loss 0.05579303205013275 - f1-score (micro avg)  0.8314\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:08:20,127 BAD EPOCHS (no improvement): 2\n2022-11-14 10:08:20,128 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:08:22,131 epoch 16 - iter 14/142 - loss 0.05219561 - samples/sec: 224.11 - lr: 0.080000\n2022-11-14 10:08:23,880 epoch 16 - iter 28/142 - loss 0.05566263 - samples/sec: 256.57 - lr: 0.080000\n2022-11-14 10:08:25,700 epoch 16 - iter 42/142 - loss 0.05528483 - samples/sec: 246.48 - lr: 0.080000\n2022-11-14 10:08:27,479 epoch 16 - iter 56/142 - loss 0.05429878 - samples/sec: 252.20 - lr: 0.080000\n2022-11-14 10:08:29,331 epoch 16 - iter 70/142 - loss 0.05385605 - samples/sec: 242.34 - lr: 0.080000\n2022-11-14 10:08:31,101 epoch 16 - iter 84/142 - loss 0.05441628 - samples/sec: 253.49 - lr: 0.080000\n2022-11-14 10:08:33,011 epoch 16 - iter 98/142 - loss 0.05514218 - samples/sec: 234.77 - lr: 0.080000\n2022-11-14 10:08:34,848 epoch 16 - iter 112/142 - loss 0.05543856 - samples/sec: 244.26 - lr: 0.080000\n2022-11-14 10:08:36,645 epoch 16 - iter 126/142 - loss 0.05541423 - samples/sec: 249.61 - lr: 0.080000\n2022-11-14 10:08:38,466 epoch 16 - iter 140/142 - loss 0.05424977 - samples/sec: 246.47 - lr: 0.080000\n2022-11-14 10:08:38,685 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:08:38,686 EPOCH 16 done: loss 0.0544 - lr 0.080000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 23/23 [00:04<00:00,  5.48it/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:08:42,895 Evaluating as a multi-label problem: False\n2022-11-14 10:08:42,910 DEV : loss 0.05537836253643036 - f1-score (micro avg)  0.8373\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:08:42,992 BAD EPOCHS (no improvement): 0\n2022-11-14 10:08:42,994 saving best model\n2022-11-14 10:08:46,969 ----------------------------------------------------------------------------------------------------\n2022-11-14 10:08:46,971 loading file resources/taggers/sota-ner-flair/best-model.pt\n2022-11-14 10:08:48,518 SequenceTagger predicts: Dictionary with 51 tags: O, S-Country, B-Country, E-Country, I-Country, S-State, B-State, E-State, I-State, S-City/town, B-City/town, E-City/town, I-City/town, S-Island, B-Island, E-Island, I-Island, S-County, B-County, E-County, I-County, S-Human-made Point-of-Interest, B-Human-made Point-of-Interest, E-Human-made Point-of-Interest, I-Human-made Point-of-Interest, S-District, B-District, E-District, I-District, S-Natural Point-of-Interest, B-Natural Point-of-Interest, E-Natural Point-of-Interest, I-Natural Point-of-Interest, S-Continent, B-Continent, E-Continent, I-Continent, S-Neighborhood, B-Neighborhood, E-Neighborhood, I-Neighborhood, S-Road/street, B-Road/street, E-Road/street, I-Road/street, S-Other locations, B-Other locations, E-Other locations, I-Other locations, <START>\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:03<00:00,  4.32it/s]","output_type":"stream"},{"name":"stdout","text":"2022-11-14 10:08:52,665 Evaluating as a multi-label problem: False\n2022-11-14 10:08:52,679 0.8385\t0.8175\t0.8279\t0.7455\n2022-11-14 10:08:52,680 \nResults:\n- F-score (micro) 0.8279\n- F-score (macro) 0.393\n- Accuracy 0.7455\n\nBy class:\n                              precision    recall  f1-score   support\n\n                     Country     0.9453    0.9268    0.9360       205\n                   City/town     0.6871    0.8000    0.7393       140\n                       State     0.8824    0.8219    0.8511       146\n                      Island     0.8485    0.8485    0.8485        33\n                      County     0.7143    0.8333    0.7692         6\nHuman-made Point-of-Interest     0.0000    0.0000    0.0000        10\n   Natural Point-of-Interest     0.0000    0.0000    0.0000         7\n                   Continent     0.6667    0.5000    0.5714         4\n                    District     0.0000    0.0000    0.0000         3\n             Other locations     0.0000    0.0000    0.0000         2\n                Neighborhood     0.0000    0.0000    0.0000         2\n                 Road/street     0.0000    0.0000    0.0000         1\n\n                   micro avg     0.8385    0.8175    0.8279       559\n                   macro avg     0.3953    0.3942    0.3930       559\n                weighted avg     0.8117    0.8175    0.8131       559\n\n2022-11-14 10:08:52,680 ----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'test_score': 0.8278985507246378,\n 'dev_score_history': [0.5379959650302621,\n  0.7531436135009927,\n  0.779016393442623,\n  0.7899686520376177,\n  0.7979539641943734,\n  0.806366047745358,\n  0.8236040609137055,\n  0.8140520896426408,\n  0.7944636678200693,\n  0.8309114927344782,\n  0.8092409240924092,\n  0.8125819134993447,\n  0.8347266881028939,\n  0.8254545454545456,\n  0.8313773934527487,\n  0.8372686662412252],\n 'train_loss_history': [0.2619265969410788,\n  0.12838247269544426,\n  0.10402785370448052,\n  0.09264882158382008,\n  0.0852409312038254,\n  0.08006780225204141,\n  0.07475391906159923,\n  0.07143794368417215,\n  0.06782041281350115,\n  0.06547560123382304,\n  0.0631039529859215,\n  0.06136859215317284,\n  0.0587967874320589,\n  0.05585249289286439,\n  0.05542128599663147,\n  0.05436841110678481],\n 'dev_loss_history': [0.15231506526470184,\n  0.09187287092208862,\n  0.0807940810918808,\n  0.07420500367879868,\n  0.06877637654542923,\n  0.06611115485429764,\n  0.06359512358903885,\n  0.06726896017789841,\n  0.0658353939652443,\n  0.060830965638160706,\n  0.05924300476908684,\n  0.057672467082738876,\n  0.05732309818267822,\n  0.060734156519174576,\n  0.05579303205013275,\n  0.05537836253643036]}"},"metadata":{}}]},{"cell_type":"code","source":"# del tagger,trainer,embeddings","metadata":{"execution":{"iopub.status.busy":"2022-11-14T10:08:52.692838Z","iopub.execute_input":"2022-11-14T10:08:52.693520Z","iopub.status.idle":"2022-11-14T10:08:52.698718Z","shell.execute_reply.started":"2022-11-14T10:08:52.693481Z","shell.execute_reply":"2022-11-14T10:08:52.697649Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Transformer","metadata":{}},{"cell_type":"code","source":"# label_type = 'ner'\n\n# label_dict = corpus.make_label_dictionary(label_type=label_type)\n# print(label_dict)\n\n# embeddings = TransformerWordEmbeddings(model='xlm-roberta-large',\n#                                        layers=\"-1\",\n#                                        subtoken_pooling=\"first\",\n#                                        fine_tune=True,\n#                                        use_context=True,\n#                                        )\n\n# tagger = SequenceTagger(hidden_size=256,\n#                         embeddings=embeddings,\n#                         tag_dictionary=label_dict,\n#                         tag_type='ner',\n#                         use_crf=False,\n#                         use_rnn=False,\n#                         reproject_embeddings=False,\n#                         )\n\n# trainer = ModelTrainer(tagger, corpus)\n\n# trainer.fine_tune('resources/taggers/sota-ner-flair',\n#               learning_rate=0.00085,\n#               mini_batch_size=16,\n#               max_epochs=8)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T10:08:52.700437Z","iopub.execute_input":"2022-11-14T10:08:52.700861Z","iopub.status.idle":"2022-11-14T10:08:52.711675Z","shell.execute_reply.started":"2022-11-14T10:08:52.700822Z","shell.execute_reply":"2022-11-14T10:08:52.710520Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## INFERENCE","metadata":{}},{"cell_type":"code","source":"model = SequenceTagger.load('./resources/taggers/sota-ner-flair/final-model.pt') #loading model","metadata":{"execution":{"iopub.status.busy":"2022-11-14T10:08:52.713865Z","iopub.execute_input":"2022-11-14T10:08:52.714759Z","iopub.status.idle":"2022-11-14T10:08:54.855528Z","shell.execute_reply.started":"2022-11-14T10:08:52.714721Z","shell.execute_reply":"2022-11-14T10:08:54.854520Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"2022-11-14 10:08:52,724 loading file ./resources/taggers/sota-ner-flair/final-model.pt\n2022-11-14 10:08:54,297 SequenceTagger predicts: Dictionary with 51 tags: O, S-Country, B-Country, E-Country, I-Country, S-State, B-State, E-State, I-State, S-City/town, B-City/town, E-City/town, I-City/town, S-Island, B-Island, E-Island, I-Island, S-County, B-County, E-County, I-County, S-Human-made Point-of-Interest, B-Human-made Point-of-Interest, E-Human-made Point-of-Interest, I-Human-made Point-of-Interest, S-District, B-District, E-District, I-District, S-Natural Point-of-Interest, B-Natural Point-of-Interest, E-Natural Point-of-Interest, I-Natural Point-of-Interest, S-Continent, B-Continent, E-Continent, I-Continent, S-Neighborhood, B-Neighborhood, E-Neighborhood, I-Neighborhood, S-Road/street, B-Road/street, E-Road/street, I-Road/street, S-Other locations, B-Other locations, E-Other locations, I-Other locations, <START>\n","output_type":"stream"}]},{"cell_type":"code","source":"folder_to_check = '../input/twitter-loc/gold-random-json/kerala_floods_2018/' # END PATH WITH / symbol \n\nimport os\n\nfor file in glob(folder_to_check + \"*\"):\n    if \"train\" not in file:\n        continue\n    for line in open(file,'r'):\n        obj = json.loads(line)\n        s = Sentence(obj[\"text\"],use_tokenizer = TokenizerWrapper(wordpunct_tokenize))\n        model.predict(s) #model is being used to predict on sentence s\n        outp =  {}\n        outp[\"tweet_id\"] = obj[\"tweet_id\"]\n        outp[\"location_mentions\"] = []\n        for e in s.labels:\n            outp[\"location_mentions\"].append({\"text\":e.data_point.text,\"start_offset\":e.data_point.start_position,\"end_offset\":e.data_point.end_position})\n        with open(\"prediction.jsonl\",'a') as out:\n            json.dump(json.dumps(outp),out)\n            out.write('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-11-14T10:08:54.857167Z","iopub.execute_input":"2022-11-14T10:08:54.857811Z","iopub.status.idle":"2022-11-14T10:09:58.101144Z","shell.execute_reply.started":"2022-11-14T10:08:54.857774Z","shell.execute_reply":"2022-11-14T10:09:58.100107Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"numobs = 5\n\nprint(\"\\033[1m Predictions \\033[0m\")\ni = 0\n\nfor line in open(\"prediction.jsonl\",'r'):\n    print(json.loads(line))\n    print()\n    i += 1\n    if i == numobs:\n        break\n\nprint('\\033[1m Ground Truth \\033[0m')\ni = 0\n\nfor line in open(folder_to_check + \"train.jsonl\"):\n    #print(line)\n    i += 1\n    if i == numobs:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-11-14T10:09:58.104437Z","iopub.execute_input":"2022-11-14T10:09:58.104848Z","iopub.status.idle":"2022-11-14T10:09:58.114815Z","shell.execute_reply.started":"2022-11-14T10:09:58.104810Z","shell.execute_reply":"2022-11-14T10:09:58.113824Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\u001b[1m Predictions \u001b[0m\n{\"tweet_id\": \"1032071697221005312\", \"location_mentions\": [{\"text\": \"Kerala\", \"start_offset\": 58, \"end_offset\": 64}, {\"text\": \"Kerala\", \"start_offset\": 204, \"end_offset\": 210}, {\"text\": \"Karnataka\", \"start_offset\": 223, \"end_offset\": 232}]}\n\n{\"tweet_id\": \"1034334390195822592\", \"location_mentions\": []}\n\n{\"tweet_id\": \"1030975049510137856\", \"location_mentions\": [{\"text\": \"Kerala\", \"start_offset\": 44, \"end_offset\": 50}]}\n\n{\"tweet_id\": \"1034481352341692417\", \"location_mentions\": [{\"text\": \"Kerala\", \"start_offset\": 107, \"end_offset\": 113}, {\"text\": \"India\", \"start_offset\": 259, \"end_offset\": 264}]}\n\n{\"tweet_id\": \"1031529205673078784\", \"location_mentions\": [{\"text\": \"Kerala\", \"start_offset\": 0, \"end_offset\": 6}, {\"text\": \"india\", \"start_offset\": 93, \"end_offset\": 98}, {\"text\": \"kerala\", \"start_offset\": 100, \"end_offset\": 106}, {\"text\": \"Kerala\", \"start_offset\": 208, \"end_offset\": 214}]}\n\n\u001b[1m Ground Truth \u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}